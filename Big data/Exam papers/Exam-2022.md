1. (2 points) Assume you are designing a NoSQL database to store students’ profiles. Each student has a unique ID and some extra information, which are not fixed among students. What data model do you use in your database? Moreover, assume you want to use consistent hashing to store students’ profiles across computers. Explain how you can use consistent hashing to spread data in your network.

**Concepts:**
[[Databases]]
Consistent hashing: [[Cassandra]]

**Answer:**
Unique ID, extra different information for each student -> Varying attributes for each student.
A wide-column store(Cassandra) or a document-oriented database(MongoDB)

*Data partition*:
Each student profile (with a unique ID) can be considered as a data point.
Apply a consistent hashing algorithm to map each student's ID to a position on a virtual ring or continuum.
`partition = hash(data) mode size(id space)`
For fault tolerance, you can replicate data on multiple nodes. Each student's data can be stored on multiple nodes, not just one.

![[Pasted image 20231024154011.png]]

2. **MapReduce**
![[Pasted image 20231024183151.png]]
**Answer:**
**Map Phase:** In the Map phase, we will distribute the processing of all the blogs to a set of worker nodes. Each worker node will process a portion of the data. The primary goal of the Map phase is to tokenize the text, count the frequency of each word, and group words by their length (number of characters). This phase will produce key-value pairs, where the key is the word length (e.g., 1 for one-character words, 2 for two-character words), and the value is the word itself and its count.
(key, value('word', count)): (1, ('a', 3)) (2, ('am', 4))

**Reduce Phase:** In the Reduce phase, we will take the intermediate key-value pairs generated in the Map phase and aggregate them. Specifically, we will group key-value pairs by the word length and sum the counts for each word length. This phase will yield the final statistics you requested, showing the count of one-character words, two-character words, and so on across all blogs.



3. **Spark questions** (8 points) 
(a) Draw the lineage graph for the following code and mention which connections are narrow and which ones are wide? 
```scala
val a = sc.parallelize(...) 
val c = sc.parallelize(...) 
val e = sc.parallelize(...) 
val b = a.groupby(...) 
val d = c.map(...) 
val f = d.union(e) 
val g = b.join(f) 
```

(b) Explain briefly how does spark join two tables, if 
i. both tables are so big that none of them can be loaded into memory of one computer
ii. one table is big and the other one is small, such that only the small one can be loaded in memory of one computer.

**Answer:**
(a) 略
(b) 
i. If both tables are large:
1. **Partitioning**: Both tables are partitioned into smaller chunks, and these partitions are distributed across multiple worker nodes. This partitioning is typically based on a partitioning key or a hash of the join columns.
2. **Shuffling**: The partitions are shuffled across the network to ensure that matching records are co-located on the same worker nodes. This step may involve network communication and data transfer.
3. **Join Operation**: Each worker node performs a local join on its partition of data. This means that the join operation is executed independently on each partition, and only the relevant data is involved in the local join.
4. **Aggregation**: The results of the local joins are collected and aggregated, resulting in the final joined dataset. This may require additional shuffling and data movement.

ii. If one table is big, another is small enough to fit in memory:
broadcast join:
1. **Broadcasting**: The smaller table is broadcasted to all worker nodes. This means that a copy of the smaller table is made available in the memory of each worker node.
2. **Partitioning**: The larger table is partitioned and distributed to worker nodes as usual.
3. **Local Join**: Each worker node performs a local join between the broadcasted small table and its partition of the large table. Since the small table is in memory on every worker node, this operation is efficient and doesn't require network shuffling.



(2 points) Explain the difference between the Transformations and Actions in Spark. Moreover, explain how Spark handles failures.
**Concepts**: Spark
**Answer:** 
Transformations and Actions in Apache Spark are two fundamental types of operations used in Spark's Resilient Distributed Dataset (RDD) programming model.
1. **Transformations**:
    
    - Transformations are operations that produce a new RDD from one or more existing RDDs.
    - They are **lazy** operations, meaning they are not executed immediately but instead build a lineage of transformations.
    - Examples of transformations include `map`, `filter`, `reduceByKey`, `join`, and `groupByKey`.
    - Transformations are used to define the sequence of data processing steps and build a directed acyclic graph (DAG) of the computation.
2. **Actions**:
    
    - Actions are operations that trigger the execution of transformations and return results to the driver program or write data to external storage.
    - They are **eager** operations, and when called, they trigger the evaluation of the lineage and the actual computation of the data.
    - Examples of actions include `count`, `collect`, `saveAsTextFile`, `reduce`, and `foreach`.
    - Actions are used to extract data or perform some final operation on the RDD.

*How Spark uses the lineage graph to handle failures?*
![[Pasted image 20231024212022.png]]


(1 points) Please list the differences between stateful and stateless operations in streaming processing programming models. Give two examples of each operation.
**Answer:**
*Differences:*
In summary, the primary difference between stateful and stateless operations in streaming processing lies in how they handle and maintain data across time. Stateful operations remember and manage data from previous batches, while stateless operations process each batch independently without maintaining any context from past or future data. The choice between stateful and stateless operations depends on the specific requirements of the streaming application.

*Stateful Stream Operations:*
`mapWithState`:
It is executed only on set of keys that are available in the last micro batch.
`UpdateStateByKey`:
It is often used for maintaining cumulative counters or aggregations.
`Windowed Aggregations`:
Operations like windowed counts or sums, where you maintain data within a specific time window, are stateful. For example, calculating the sum of stock prices in a 5-minute window.

*Stateless Stream Operations:*
`map`: 
It processes each element in an RDD or DStream independently, without any knowledge of the past or future data.
`Filter`: 
It evaluates each data point independently based on a condition, without reference to past or future data.



(1 points) What type of process (ETL or ELT) is executed in Dataware house and Data lake? Briefly explain their differences.
ETL is used in Data warehouse, ELT is used in Data lake.
Data Lake defines the schema(transform) after data is loaded whereas Data Warehouse defines the schema before data is stored. 




(2 points) In an undirected graph, each of two directly connected nodes is called neighbors. Assume you have an undirected graph in which each node stores a pair (id, #neighbors), where the first item is the node’s ID, and the second one is the number of node’s neighbors. Write three pseudo-codes in the Pregel, Graphlab, and PowerGraph to find the node ID with the smallest number of neighbors. If two nodes have the same number of neighbors, choose the one with the smallest node ID.

1. Pregel
```scala
Initialize():
	min_id_neighbors = (id, #neighbors)

compute():
	if is_active():
		if (#neighbors < min_id_neighbors.#neighbors) or (#neighbors == min_id_neighbors.#neighbors and id < min_id_neighbors.id):
			min_id_neighbors = (id, #neighbors)
	vote_to_halt()
```

2. GraphLab
```scala
GraphLab_FindMinNeighbors(i)
	// Initialize variables
	min_id_neighbors = (id, #neighbors);
	// Iterate over incoming neighbors
	foreach(j in in_neighbors(i)):
		  // Compare the number of neighbors and node IDs
		  if (#neighbors < R[j].#neighbors || (#neighbors == R[j].#neighbors && id < R[j].id))
			min_id_neighbors = (id, #neighbors);
		  // Update the minimum number of neighbors
		    R[i] = min_id_neighbors;
		
		// Trigger neighbors to run again
		foreach(j in out_neighbors(i))
		  signal vertex-program on j;

```

3. PowerGraph
```scala
PowerGraph_FindMinNeighbors(i)
	// Initialize variables
	min_id_neighbors = (id, #neighbors);
	// Gather phase: Compute the minimum number of neighbors and the corresponding node ID
	Gather(j -> i)
	return (#neighbors < R[j].#neighbors) || (#neighbors == R[j].#neighbors && id < R[j].id)) ? (id, #neighbors) : R[j]; 
	// Pair (id, #neighbors)
	// Apply phase: Update the minimum number of neighbors and the corresponding node ID
	Apply(i)
	if (#neighbors < min_id_neighbors.#neighbors) || (#neighbors == min_id_neighbors.#neighbors && id < min_id_neighbors.id))
	{
	  min_id_neighbors = (id, #neighbors);
	  R[i] = min_id_neighbors;
}
	// Check for convergence and vote to halt if necessary
	if (min_id_neighbor = R[i])
		vote_to_halt
	else
	// Scatter: Signal neighbors to re-run their computations
		Scatter(i -> j)

```