1. Distributed file systems such as GFS store files in chunks. The size of a chunk has implications on the system’s performance. What are the advantages and disadvantages of storing files in large chunks? (5 points)

**Concept**: GFS
[[GFS]]
'Why Large Chunks' section


2. Explain briefly the similarities and differences of document-based and column oriented databases. What is the difference between storing data in row model and column model? When is better to use the column model? (5 points)
**Concepts**: databases
**Answer**:
- *Similarities*: 
Both are NoSQL Databases, which means they are designed to handle large volumes of unstructured or semi-structured data.

Both allow for flexible schemas, where the structure of the data can vary within the same collection or table.

Both are designed for horizontal scalability, allowing to add more servers to handle increasing data loads.

- *Difference*:
Document-based: store data in documents, in formats like JSON. Column-oriented: store data in columns rather than rows.

*When to use column model*:
Perform analytical queries that involve aggregations, filtering, and calculations on large datasets. Better for row model since it is faster when quering.

You are dealing with data warehousing, business intelligence, and data analytics use cases where performance and storage efficiency are critical.(Apache Cassandra, HBase, or Google Bigtable)

...
3. Assume we want to implement a MapReduce code to multiply a one-dimensional matrix with a two-dimensional matrix. Briefly explain what map and reduce functions should do. (5 points) 
	Reminder. The matrix product of matrices A and B is a third matrix C, where C = AB. If A is of shape m × n and B is of shape n × p, then C is of shape m × p, such that:
![[Pasted image 20231023202621.png]]
5. Explain how does using DataFrame in Spark improve the programming performance compared to RDD. What is the relation between DataFrame and DataSet in Spark? (5 points)
**Answer:**
DataFrame elements are Rows, which are generic untyped JVM objects
Scala compiler cannot type check Spark SQL schemas in DataFrames
![[Pasted image 20231024195826.png]]


6. Explain briefly how does spark join two tables, if (5 points) 
(a) both tables are so big that none of them can be loaded into memory of one computer 
(b) one table is big and the other one is small, such that only the small one can be loaded in memory of one computer. (same question in [[Exam-2022]]/Spark)


8. How can a batch system be used to process streams, and how can a streaming system be used to process batches? (5 points)
Using a batch system for processing Streams:
a. Time-based Batching: to create small time-based batches. For example, configure the batch system to create a new batch every minute, hour or day.
b. Micro-batch Processing: process data in smaller chunks. It offers a compromise between traditional batch processing and real-time stream processing.
c. Checkpoint: 
Using a Streaming System for Processing Batches:
a. Fixed Window Processing:
Define fixed windows of data to mimic batch processing
b. Aggregating Windows: 
Instead of processing data record by record, you can aggregate data within a window and then process the aggregated data as a batch.


9. Explain how Kafka provides scalability and fault tolerance? (4 points)
**Scalability:**
a. Distributed Architecture: 
it can run on a cluster of machines, where data is distributed across multiple brokers. This distribution allows Kafka to scale horizontally, adding more brokers as needed.
b. Partition: 
the topic is divided into partitions for scalability.
c. Scalable Producers and Consumers:
Kafka producers and consumers can scale independently. 
**Fault tolerance:**
a. Partition Replication: 
copying the partition data to other brokers.  Each partition have multiple replicas distributed across different brokers. If one broker fails, another replica can take over.


10. Compare the graph processing models in Pregel, GraphLab, X-Stream. (6 points)
**Pregel**:
- Pregel employs a vertex-centric approach, where computations are centered around indiviual vertices in the graph. 
- Pregel uses a synchronous model where computation progresses in fixed rounds (supersteps). Each vertex processes its own state and messages from neighboring vertices in a series of supersteps.
- It is well-suited for batch processing of static graphs. It is widely used for graph algorithms like PageRank
**GraphLab**: 
- It supports both vertex-centric and edge-centric models.
- GraphLab allows asynchoronous iterative computation
**X-Stream:**
- X-Stream focus on an edge-centric approach. It processes graph edges in a streaming fashion
- X-Stream: X-Stream uses an asynchronous and decentralized model where edges are processed in real-time as they arrive, providing low-latency processing for dynamic graphs.
- Tailored for scenarios with dynamic and evolving graphs, such as social network analysis, where edges change frequently.


11. Assume we have two types of resources in the system, i.e., CPU and Memory. In total we have 10 CPU and 20GB RAM. There are two users in the systems. User 1 needs h1CPU, 4GBi per task, and user 2 needs h2CPU, 1GBi per task. How do you share the resources fairly among these two users, considering the asset fairness and DRF. (5 points)

max(x,y)
- Asset: equalize total value given to each user
x + 2y <= 10
4x + y <= 20
6x = 5y
- DRF: equal share of its dominant resource
x + 2y <= 10
4x + y <= 20
4x/20 = 2y/10

