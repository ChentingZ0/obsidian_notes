1. **True/False questions**. Please briefly explain the reasoning behind your choice (8 points) 
(a) With MapReduce, each of the R reducers is responsible for producing 1 /R th of the amount of output data (true/false, why?) 
(b) GFS (HDFS) is used to store input, intermediate, and output files for MapReduce jobs (true/false, why?) 
(c) Assume we have a Dynamo storage with a hash function H that produces IDs between 0 and 32 to store and locate objects on the servers. If we have five servers with IDs 0, 3, 8, 18, 23 and H(X) = 26, then object X will be stored on the server with ID 23 (true/false, why?) 
(d) BigTable provides fault tolerance by replicating data on multiple tablet servers (true/false, why?)

(a) 
**Concepts**: MapReduce
True, for the reason of data partitioning and load balancing
(b) 
**Concepts**: GFS, MapReduce
True, GFS and HDFS are two distinct systems.(HDFS is inspired by GFS)
HDFS and MapReduce are tightly integrated within the Hadoop ecosystem. HDFS is commonly used to store input, intermediate, and output data for MapReduce jobs.
(c)
**Concepts**: hash function, Dynamo
True. It will be allocated the nearest available server(the closest and less than or equal to the hashed value), which is 23.

(d) 
**Concepts**: BigTable
True

2. **MapReduce questions**
Suppose that you are given two documents with the following content:
• Document1: Hello world Hello Hadoop 
• Document2: Hello Spark
We want to generate a list of locations (i.e., word number in the document and identifier for the document) for each word occurrence. The output generated by your program should look like: 
Hello → Document1 : 1, 3 | Document2 : 1 
world → Document1 : 2 
Hadoop → Document1 : 4 
Spark → Document2 : 2 
Write out in pseudo-code the steps taken in Hadoop’s map and reduce phases to generate the above output. Please also specify the input and output of the map() and reduce() functions. Assume the identifier for each document is provided as the key to the map() function.

**Answer:**
*Map function:*
```python
map(doc_id, text):
	words = text.split()
	for word in words:
		emit(word, doc_id + ":" + word_location)

map("Document1", "Hello word Hello Hadoop")
```
The result of map function is:
```python
"Hello" → "Document1 : 1"
"world" → "Document1 : 2"
"Hello" → "Document1 : 3"
"Hadoop" → "Document1 : 4"
```

*Reduce function:*
```python
reduce(word, locations):
	location_list = []
	for location in locations:
		locations_list.append(location)
	emit(word, "|".join(locations_list))

reduce("Hello", ["Document1 : 1", "Document1 : 3", "Document2 : 1"])
```
The result of reduce function is:
```python
"Hello" → "Document1 : 1, 3 | Document2 : 1"
```


3. **Spark questions**
略, same



4. **Streaming questions** (8 points) 
(a) Assume you want to use Storm to implement the word count application. Explain what spouts and bolts you need, and what types of grouping do you use between them. 
(b) Briefly compare the fault tolerance model of Storm, Spark Streaming and Flink.

No storm information learned during the course



5. **Graph questions** (8 points) 
(a) What is the difference between message passing and shared memory models in graph processing platforms? 
(b) Assume we have a graph, in which all vertices have a local numeric value. Write a vertex-centric Gather-Apply-Scatter pseudo-code to update the local value of the vertices with the minimum value in the graph. For example, if a graph has three vertices A, B, and C, with values 4, 2 and 5, respectively, we would like to end up with value 2 at all vertices.

(a). 
- **Message Passing**: In the message-passing model, vertices communicate by sending messages to their neighbors. Each vertex processes incoming messages and updates its state accordingly. Communication happens explicitly via message passing.
  Typically uses asynchronous communication
  Follows a vertex-centric model.
- **Shared Memory**: In the shared memory model, vertices share a common memory space. They can read and write data directly to this shared memory. There is no need for explicit message passing because all vertices can access the shared memory simultaneously.
  Offers more synchronization
  Offers a global programming model

```scala
Gather-Apply-Scatter_MinValue(vertex):
// Initialization
	min_neighbor_value = vertex.value
	for each neighbor in vertex.neighbors:
		if neighbor.value < min_neighbor_value:
			min_neighbor_value = neighbor.value

	// Apply Phase
	if min_neighbor_value < vertex.value: 
		vertex.value = min_neighbor_value
	// Scatter Phase
	for each neighbor in vertex.neighbors:
		signal vertex to re-run in if neighbor.value != min_neighbor_value

```

